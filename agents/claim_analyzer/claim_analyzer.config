# Claim Analyzer MCP Server Configuration
# config.yml

# API Keys for external services
api_keys:
  google_fact_check: ""  # Google Fact Check Tools API key
  bing_search: ""        # Bing Search API key
  openai: ""             # OpenAI API key for advanced NLP
  anthropic: ""          # Anthropic API key

# Trusted sources for fact-checking (domains)
trusted_sources:
  - "wikipedia.org"
  - "snopes.com"
  - "factcheck.org"
  - "politifact.com"
  - "reuters.com"
  - "bbc.com"
  - "npr.org"
  - "ap.org"
  - "cdc.gov"
  - "nasa.gov"
  - "who.int"
  - "nih.gov"
  - "nature.com"
  - "science.org"
  - "arxiv.org"

# Source credibility scoring
source_credibility:
  "wikipedia.org": 0.8
  "snopes.com": 0.95
  "factcheck.org": 0.95
  "politifact.com": 0.9
  "reuters.com": 0.9
  "bbc.com": 0.85
  "npr.org": 0.8
  "ap.org": 0.9
  "cdc.gov": 0.95
  "nasa.gov": 0.95
  "who.int": 0.9
  "nih.gov": 0.9
  "nature.com": 0.95
  "science.org": 0.95
  "arxiv.org": 0.8

# Analysis settings
analysis:
  # Language for NLP processing
  language: "english"
  
  # Maximum number of sources to return per fact-check
  max_results: 10
  
  # Minimum confidence threshold for claim detection
  confidence_threshold: 0.5
  
  # Cache duration for fact-check results (hours)
  cache_duration_hours: 24
  
  # Maximum claim length to process
  max_claim_length: 1000
  
  # Minimum claim length to process
  min_claim_length: 10

# Rate limiting settings
rate_limiting:
  # Requests per minute for external APIs
  api_requests_per_minute: 30
  
  # Delay between fact-check requests (seconds)
  fact_check_delay: 2.0
  
  # Maximum concurrent requests
  max_concurrent_requests: 5

# Database settings
database:
  # SQLite database path
  path: "claims.db"
  
  # Enable database logging
  enable_logging: true
  
  # Maximum number of cached results
  max_cached_results: 10000
  
  # Cleanup interval (hours)
  cleanup_interval_hours: 24

# Logging configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  
  # Log to file
  log_to_file: true
  log_file: "claim_analyzer.log"
  
  # Max log file size (MB)
  max_log_size_mb: 100
  
  # Number of backup log files
  backup_count: 5

# NLP model settings
nlp:
  # spaCy model to use
  spacy_model: "en_core_web_sm"
  
  # Sentence transformer model for similarity
  sentence_model: "all-MiniLM-L6-v2"
  
  # Enable GPU acceleration if available
  use_gpu: false
  
  # Batch size for processing
  batch_size: 32

# Fact-checking behavior
fact_checking:
  # Enable web search for evidence
  enable_web_search: true
  
  # Enable fact-checking API integration
  enable_fact_check_apis: true
  
  # Enable cached claim lookup
  enable_cache_lookup: true
  
  # Require minimum number of sources for verdict
  min_sources_for_verdict: 2
  
  # Handle conflicting evidence
  handle_conflicts: true
  
  # Default verdict for insufficient evidence
  default_verdict: "Unverified"

# Performance settings
performance:
  # Enable async processing
  enable_async: true
  
  # Worker pool size
  worker_pool_size: 4
  
  # Memory limit for processing (MB)
  memory_limit_mb: 2048
  
  # Timeout for external requests (seconds)
  request_timeout: 30

# Security settings
security:
  # Enable request validation
  enable_validation: true
  
  # Maximum request size (bytes)
  max_request_size: 1048576  # 1MB
  
  # Enable rate limiting per client
  enable_client_rate_limiting: true
  
  # Sanitize input text
  sanitize_input: true

# Feature flags
features:
  # Enable semantic similarity search
  semantic_similarity: true
  
  # Enable pattern-based claim detection
  pattern_detection: true
  
  # Enable entity recognition
  entity_recognition: true
  
  # Enable sentiment analysis
  sentiment_analysis: false
  
  # Enable multilingual support
  multilingual: false
  
  # Enable real-time analysis
  real_time_analysis: true

# Experimental features (use with caution)
experimental:
  # Use LLM for claim analysis
  llm_analysis: false
  
  # Enable machine learning model training
  ml_training: false
  
  # Use vector database for similarity search
  vector_database: false
  
  # Enable cross-domain pattern detection
  cross_domain_patterns: false

# MCP Server settings
mcp:
  # Server name
  name: "claim-analyzer"
  
  # Server version
  version: "1.0.0"
  
  # Description
  description: "AI-powered claim analysis and fact-checking server"
  
  # Transport method (stdio, http, websocket)
  transport: "stdio"
  
  # HTTP server settings (if using HTTP transport)
  http:
    host: "localhost"
    port: 8000
    
  # Enable resource sharing
  enable_resources: true
  
  # Enable tool registration
  enable_tools: true
  
  # Maximum concurrent connections
  max_connections: 100

# Custom claim patterns (regex)
custom_patterns:
  # Patterns for identifying claims
  claim_indicators:
    - "(?:is|are|was|were|will be|has been|have been)\\s+(?:a|an|the)?\\s*\\w+"
    - "(?:claims?|states?|argues?|believes?|says?)\\s+that\\s+.*"
    - "(?:according to|research shows|studies indicate|experts say)\\s+.*"
    - "(?:it is|this is|that is)\\s+(?:true|false|correct|incorrect|a fact)\\s+that\\s+.*"
    - "\\b(?:always|never|all|none|every|no one|everyone)\\b"
  
  # Patterns for identifying non-claims
  non_claim_indicators:
    - "^\\s*[Qq]uestion:"
    - "^\\s*[Hh]ow to"
    - "^\\s*[Ww]hat if"
    - "^\\s*[Cc]an you"
    - "\\?\\s*$"

# Output formatting
output:
  # Include confidence scores in output
  include_confidence: true
  
  # Include source URLs
  include_sources: true
  
  # Include reasoning explanation
  include_reasoning: true
  
  # Maximum length of reasoning text
  max_reasoning_length: 500
  
  # Format for timestamps
  timestamp_format: "%Y-%m-%d %H:%M:%S"
  
  # Include metadata in responses
  include_metadata: true
