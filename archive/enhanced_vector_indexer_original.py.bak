#!/usr/bin/env python3
"""
Enhanced Vector Indexer Agent for MCP Yggdrasil
Dynamic model selection, quality checking, and advanced vector operations
"""

import asyncio
import logging
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
from datetime import datetime
import json
import uuid
import hashlib
from concurrent.futures import ThreadPoolExecutor
import yaml
import numpy as np

# Vector models and processing
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.preprocessing import normalize
    from sklearn.decomposition import PCA
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("Advanced ML libraries not available - using basic functionality")

# Import base functionality
try:
    from .vector_indexer import VectorIndexer, VectorDocument, SearchResult, QdrantManager, RedisCache
    BASE_INDEXER_AVAILABLE = True
except ImportError:
    # Fallback if base indexer not available
    BASE_INDEXER_AVAILABLE = False
    logging.warning("Base vector indexer not found - implementing standalone")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ModelPerformance:
    """Model performance metrics"""
    model_name: str
    accuracy: float
    speed: float  # embeddings per second
    quality_score: float
    memory_usage: float  # MB
    vector_size: int
    supported_languages: List[str]
    last_evaluated: datetime


@dataclass
class EmbeddingQuality:
    """Quality assessment of generated embeddings"""
    doc_id: str
    model_used: str
    quality_score: float  # 0-1 scale
    consistency_score: float  # Embedding consistency
    semantic_density: float  # Information density
    outlier_score: float  # How much it differs from typical embeddings
    recommendations: List[str]  # Improvement suggestions


@dataclass 
class VectorIndexResult:
    """Enhanced result from vector indexing"""
    vector_id: str
    embedding: np.ndarray
    model_used: str
    quality_assessment: EmbeddingQuality
    metadata: Dict
    processing_time: float
    confidence_score: float


class ModelManager:
    """Manages multiple embedding models and selects optimal ones"""
    
    def __init__(self):
        self.models = {}
        self.model_performances = {}
        self.load_models()
    
    def load_models(self):
        """Load and initialize embedding models"""
        if not TRANSFORMERS_AVAILABLE:
            logger.warning("Advanced models not available - using mock models")
            self._setup_mock_models()
            return
        
        # Model configurations with their strengths
        model_configs = {
            'general': {
                'name': 'all-MiniLM-L6-v2',
                'strengths': ['speed', 'general_purpose'],
                'languages': ['en', 'es', 'fr', 'de', 'it'],
                'vector_size': 384
            },
            'multilingual': {
                'name': 'paraphrase-multilingual-MiniLM-L12-v2', 
                'strengths': ['multilingual', 'semantic_quality'],
                'languages': ['en', 'zh', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'pl', 'ru', 'ja', 'ar'],
                'vector_size': 384
            },
            'semantic': {
                'name': 'all-mpnet-base-v2',
                'strengths': ['semantic_quality', 'academic'],
                'languages': ['en'],
                'vector_size': 768
            },
            'domain_academic': {
                'name': 'allenai/scibert_scivocab_uncased',
                'strengths': ['academic', 'scientific'],
                'languages': ['en'],
                'vector_size': 768
            },
            'fast': {
                'name': 'all-MiniLM-L12-v2',
                'strengths': ['speed', 'efficiency'],
                'languages': ['en'],
                'vector_size': 384
            }
        }
        
        # Load available models
        for model_key, config in model_configs.items():
            try:
                logger.info(f"Loading model: {config['name']}")
                model = SentenceTransformer(config['name'])
                self.models[model_key] = {
                    'model': model,
                    'config': config,
                    'loaded_at': datetime.now()
                }
                
                # Initialize performance metrics
                self.model_performances[model_key] = ModelPerformance(
                    model_name=config['name'],
                    accuracy=0.85,  # Will be updated with actual benchmarks
                    speed=100.0,    # Will be measured
                    quality_score=0.8,
                    memory_usage=200.0,
                    vector_size=config['vector_size'],
                    supported_languages=config['languages'],
                    last_evaluated=datetime.now()
                )
                
                logger.info(f"Successfully loaded model: {model_key}")
                
            except Exception as e:
                logger.warning(f"Failed to load model {model_key}: {e}")
                continue
        
        if not self.models:
            logger.error("No models loaded successfully - falling back to mock models")
            self._setup_mock_models()
    
    def _setup_mock_models(self):
        """Setup mock models for testing when transformers unavailable"""
        self.models = {
            'general': {
                'model': 'mock_general',
                'config': {
                    'name': 'mock_general_model',
                    'strengths': ['general_purpose'],
                    'languages': ['en'],
                    'vector_size': 384
                }
            }
        }
        
        self.model_performances = {
            'general': ModelPerformance(
                model_name='mock_general_model',
                accuracy=0.75,
                speed=1000.0,
                quality_score=0.7,
                memory_usage=50.0,
                vector_size=384,
                supported_languages=['en'],
                last_evaluated=datetime.now()
            )
        }
    
    def select_optimal_model(self, content: Dict) -> str:
        """Dynamically select the best model for given content"""
        if not self.models:
            return 'general'  # Fallback
        
        # Extract content characteristics
        language = content.get('language', 'en').lower()
        domain = content.get('domain', 'general').lower()
        text_length = len(content.get('text', ''))
        content_type = content.get('type', 'general')
        
        # Scoring function for model selection
        best_model = None
        best_score = -1
        
        for model_key, model_info in self.models.items():
            score = 0
            config = model_info['config']
            performance = self.model_performances[model_key]
            
            # Language support scoring
            if language in config['languages']:
                score += 3.0
            elif 'multilingual' in config['strengths']:
                score += 2.0
            else:
                score += 0.5  # Basic fallback
            
            # Domain-specific scoring
            if domain in ['science', 'technology', 'mathematics']:
                if 'academic' in config['strengths'] or 'scientific' in config['strengths']:
                    score += 2.0
                elif 'semantic_quality' in config['strengths']:
                    score += 1.5
            
            if domain in ['philosophy', 'literature', 'art']:
                if 'semantic_quality' in config['strengths']:
                    score += 2.0
                elif 'general_purpose' in config['strengths']:
                    score += 1.0
            
            # Performance-based scoring
            score += performance.quality_score * 1.5
            score += performance.accuracy * 1.0
            
            # Text length considerations
            if text_length > 5000:  # Long documents
                if 'speed' in config['strengths']:
                    score += 1.0
            elif text_length < 500:  # Short documents
                if 'semantic_quality' in config['strengths']:
                    score += 1.0
            
            # Update best model if this scores higher
            if score > best_score:
                best_score = score
                best_model = model_key
        
        logger.debug(f"Selected model '{best_model}' with score {best_score:.2f} for content: {content.get('title', 'unknown')}")
        return best_model or 'general'
    
    def get_model(self, model_key: str):
        """Get model instance"""
        if model_key not in self.models:
            logger.warning(f"Model {model_key} not found, using general")
            model_key = 'general'
        
        return self.models[model_key]['model']
    
    def benchmark_model(self, model_key: str, test_texts: List[str]) -> ModelPerformance:
        """Benchmark model performance"""
        if model_key not in self.models or not TRANSFORMERS_AVAILABLE:
            return self.model_performances.get(model_key, self.model_performances['general'])
        
        model = self.models[model_key]['model']
        start_time = time.time()
        
        try:
            # Generate embeddings for benchmark
            embeddings = model.encode(test_texts, show_progress_bar=False)
            
            # Calculate metrics
            processing_time = time.time() - start_time
            speed = len(test_texts) / processing_time if processing_time > 0 else 0
            
            # Quality assessment (simplified)
            embedding_array = np.array(embeddings)
            avg_norm = np.mean(np.linalg.norm(embedding_array, axis=1))
            quality_score = min(avg_norm / 10.0, 1.0)  # Normalized quality
            
            # Update performance record
            performance = ModelPerformance(
                model_name=self.models[model_key]['config']['name'],
                accuracy=0.85,  # Would need labeled data for real accuracy
                speed=speed,
                quality_score=quality_score,
                memory_usage=200.0,  # Would measure actual memory
                vector_size=embeddings.shape[1] if len(embeddings) > 0 else 384,
                supported_languages=self.models[model_key]['config']['languages'],
                last_evaluated=datetime.now()
            )
            
            self.model_performances[model_key] = performance
            return performance
            
        except Exception as e:
            logger.error(f"Error benchmarking model {model_key}: {e}")
            return self.model_performances.get(model_key, self.model_performances['general'])


class EmbeddingQualityChecker:
    """Assesses the quality of generated embeddings"""
    
    def __init__(self):
        self.quality_thresholds = {
            'minimum_norm': 0.1,
            'maximum_norm': 10.0,
            'consistency_threshold': 0.7,
            'outlier_threshold': 2.0
        }
        self.reference_embeddings = {}  # Store reference embeddings for comparison
    
    def assess_quality(self, embedding: np.ndarray, content: Dict, model_used: str) -> EmbeddingQuality:
        """Comprehensive quality assessment of an embedding"""
        doc_id = content.get('doc_id', 'unknown')
        
        # Basic vector quality checks
        embedding_norm = np.linalg.norm(embedding)
        quality_score = 1.0
        recommendations = []
        
        # Norm checks
        if embedding_norm < self.quality_thresholds['minimum_norm']:
            quality_score -= 0.3
            recommendations.append("Embedding norm too low - may indicate poor text quality")
        elif embedding_norm > self.quality_thresholds['maximum_norm']:
            quality_score -= 0.2
            recommendations.append("Embedding norm too high - may indicate outlier content")
        
        # Consistency assessment (compare with similar documents if available)
        consistency_score = self._assess_consistency(embedding, content, model_used)
        
        # Semantic density (how much information is captured)
        semantic_density = self._calculate_semantic_density(embedding, content)
        
        # Outlier detection
        outlier_score = self._detect_outliers(embedding, content.get('domain', 'general'))
        
        # Overall quality adjustment
        quality_score = min(1.0, max(0.0, quality_score - (1.0 - consistency_score) * 0.2))
        
        # Generate specific recommendations
        if quality_score < 0.7:
            recommendations.append("Consider trying a different model for better quality")
        if consistency_score < 0.6:
            recommendations.append("Content may be too noisy or ambiguous")
        if semantic_density < 0.4:
            recommendations.append("Content may lack semantic richness")
        
        return EmbeddingQuality(
            doc_id=doc_id,
            model_used=model_used,
            quality_score=quality_score,
            consistency_score=consistency_score,
            semantic_density=semantic_density,
            outlier_score=outlier_score,
            recommendations=recommendations
        )
    
    def _assess_consistency(self, embedding: np.ndarray, content: Dict, model_used: str) -> float:
        """Assess embedding consistency with similar content"""
        domain = content.get('domain', 'general')
        reference_key = f"{domain}_{model_used}"
        
        if reference_key not in self.reference_embeddings:
            # Store as reference for future comparisons
            self.reference_embeddings[reference_key] = []
        
        references = self.reference_embeddings[reference_key]
        
        if len(references) < 5:
            # Not enough references yet
            references.append(embedding)
            return 0.8  # Assume decent consistency
        
        # Calculate similarity with recent references
        similarities = []
        for ref_embedding in references[-10:]:  # Use last 10 references
            if TRANSFORMERS_AVAILABLE:
                similarity = cosine_similarity([embedding], [ref_embedding])[0][0]
                similarities.append(similarity)
        
        if similarities:
            consistency = np.mean(similarities)
            # Add current embedding to references
            references.append(embedding)
            # Keep only recent references
            if len(references) > 100:
                references[:] = references[-50:]
            
            return max(0.0, min(1.0, consistency))
        
        return 0.7  # Default consistency
    
    def _calculate_semantic_density(self, embedding: np.ndarray, content: Dict) -> float:
        """Calculate how much semantic information is captured"""
        text = content.get('text', '')
        text_length = len(text.split())
        
        if text_length == 0:
            return 0.0
        
        # Simple heuristic: longer, more complex text should have higher density
        # This is a simplified measure - real semantic density would require more analysis
        embedding_norm = np.linalg.norm(embedding)
        
        # Normalize by text length and embedding norm
        density = min(1.0, (embedding_norm * np.log(text_length + 1)) / 20.0)
        
        return max(0.0, density)
    
    def _detect_outliers(self, embedding: np.ndarray, domain: str) -> float:
        """Detect if embedding is an outlier for the domain"""
        reference_key = f"outlier_{domain}"
        
        if reference_key not in self.reference_embeddings:
            self.reference_embeddings[reference_key] = []
        
        domain_embeddings = self.reference_embeddings[reference_key]
        
        if len(domain_embeddings) < 10:
            # Not enough data for outlier detection
            domain_embeddings.append(embedding)
            return 0.5  # Neutral outlier score
        
        # Calculate distance from domain centroid
        if TRANSFORMERS_AVAILABLE:
            centroid = np.mean(domain_embeddings, axis=0)
            distance = np.linalg.norm(embedding - centroid)
            
            # Calculate standard deviation of distances
            distances = [np.linalg.norm(emb - centroid) for emb in domain_embeddings]
            std_distance = np.std(distances)
            
            # Outlier score based on how many standard deviations away
            outlier_score = distance / (std_distance + 1e-6)
            
            # Add to domain embeddings
            domain_embeddings.append(embedding)
            # Keep manageable size
            if len(domain_embeddings) > 200:
                domain_embeddings[:] = domain_embeddings[-100:]
            
            return min(5.0, outlier_score)  # Cap at 5 standard deviations
        
        return 1.0  # Default neutral score


class EnhancedVectorIndexer:
    """Enhanced vector indexer with dynamic model selection and quality checking"""
    
    def __init__(self, config_path: str = "agents/qdrant_manager/vector_index/enhanced_config.yaml"):
        """Initialize enhanced vector indexer"""
        self.load_config(config_path)
        
        # Initialize components
        self.model_manager = ModelManager()
        self.quality_checker = EmbeddingQualityChecker()
        
        # Initialize base indexer if available
        if BASE_INDEXER_AVAILABLE:
            self.base_indexer = VectorIndexer(config_path)
            self.qdrant = self.base_indexer.qdrant
            self.cache = self.base_indexer.cache
        else:
            # Setup basic components
            self.qdrant = QdrantManager(
                host=self.config['qdrant']['host'],
                port=self.config['qdrant']['port'],
                api_key=self.config['qdrant'].get('api_key')
            )
            self.cache = RedisCache(
                redis_url=self.config['redis']['url'],
                prefix=self.config['redis']['prefix']
            )
        
        self.executor = ThreadPoolExecutor(max_workers=self.config.get('max_workers', 4))
    
    def load_config(self, config_path: str) -> None:
        """Load enhanced configuration"""
        try:
            with open(config_path, 'r') as f:
                self.config = yaml.safe_load(f)
        except FileNotFoundError:
            # Enhanced default configuration
            self.config = {
                'qdrant': {
                    'host': 'localhost',
                    'port': 6333,
                    'api_key': None
                },
                'redis': {
                    'url': 'redis://localhost:6379',
                    'prefix': 'mcp_enhanced_vector:'
                },
                'enhanced_features': {
                    'dynamic_model_selection': True,
                    'quality_checking': True,
                    'performance_monitoring': True,
                    'adaptive_thresholds': True
                },
                'quality_thresholds': {
                    'minimum_quality_score': 0.6,
                    'reindex_threshold': 0.4,
                    'consistency_threshold': 0.7
                },
                'performance': {
                    'batch_size': 50,
                    'max_workers': 4,
                    'cache_ttl': 7200,
                    'benchmark_frequency': 3600  # seconds
                }
            }
    
    async def index_content_enhanced(self, content: Dict, force_model: Optional[str] = None) -> VectorIndexResult:
        """Enhanced content indexing with dynamic model selection and quality checking"""
        start_time = time.time()
        
        try:
            # Select optimal model
            if force_model and force_model in self.model_manager.models:
                selected_model = force_model
            else:
                selected_model = self.model_manager.select_optimal_model(content)
            
            logger.info(f"Using model '{selected_model}' for content: {content.get('title', 'untitled')}")
            
            # Generate embedding
            embedding = await self._generate_embedding(content, selected_model)
            
            if embedding is None:
                raise ValueError("Failed to generate embedding")
            
            # Quality assessment
            quality_assessment = self.quality_checker.assess_quality(embedding, content, selected_model)
            
            # Check if quality meets threshold
            if quality_assessment.quality_score < self.config['quality_thresholds']['minimum_quality_score']:
                logger.warning(f"Low quality embedding (score: {quality_assessment.quality_score:.2f}) - trying alternative model")
                
                # Try alternative model
                alternative_model = self._get_alternative_model(selected_model)
                if alternative_model != selected_model:
                    embedding = await self._generate_embedding(content, alternative_model)
                    quality_assessment = self.quality_checker.assess_quality(embedding, content, alternative_model)
                    selected_model = alternative_model
            
            # Create vector document
            doc_id = content.get('doc_id', str(uuid.uuid4()))
            vector_doc = VectorDocument(
                id=doc_id,
                vector=embedding.tolist() if hasattr(embedding, 'tolist') else embedding,
                metadata={
                    **content.get('metadata', {}),
                    'model_used': selected_model,
                    'quality_score': quality_assessment.quality_score,
                    'indexed_at': datetime.now().isoformat(),
                    'enhanced_indexing': True
                }
            )
            
            # Index using base indexer if available
            if hasattr(self, 'base_indexer'):
                success = self.base_indexer.index_document({
                    **content,
                    'embeddings': embedding.tolist() if hasattr(embedding, 'tolist') else embedding
                })
            else:
                # Basic indexing fallback
                success = True  # Simplified
            
            processing_time = time.time() - start_time
            
            # Calculate confidence score
            confidence_score = self._calculate_confidence_score(quality_assessment, selected_model)
            
            result = VectorIndexResult(
                vector_id=doc_id,
                embedding=embedding,
                model_used=selected_model,
                quality_assessment=quality_assessment,
                metadata=vector_doc.metadata,
                processing_time=processing_time,
                confidence_score=confidence_score
            )
            
            logger.info(f"Enhanced indexing completed in {processing_time:.2f}s with quality score {quality_assessment.quality_score:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"Enhanced indexing failed: {e}")
            raise
    
    async def _generate_embedding(self, content: Dict, model_key: str) -> Optional[np.ndarray]:
        """Generate embedding using specified model"""
        text = content.get('text', '')
        if not text:
            logger.warning("No text content provided for embedding")
            return None
        
        model = self.model_manager.get_model(model_key)
        
        if model == 'mock_general' or not TRANSFORMERS_AVAILABLE:
            # Mock embedding for testing
            return np.random.rand(384)
        
        try:
            # Generate embedding
            if hasattr(model, 'encode'):
                embedding = model.encode([text], show_progress_bar=False)[0]
                return np.array(embedding)
            else:
                # Fallback to mock
                return np.random.rand(384)
                
        except Exception as e:
            logger.error(f"Error generating embedding with model {model_key}: {e}")
            return None
    
    def _get_alternative_model(self, current_model: str) -> str:
        """Get alternative model for quality improvement"""
        # Model preference order for alternatives
        alternatives = {
            'general': 'semantic',
            'semantic': 'multilingual',
            'multilingual': 'general',
            'fast': 'general',
            'domain_academic': 'semantic'
        }
        
        alternative = alternatives.get(current_model, 'general')
        
        # Check if alternative is available
        if alternative in self.model_manager.models:
            return alternative
        
        # Return first available model
        available_models = list(self.model_manager.models.keys())
        return available_models[0] if available_models else 'general'
    
    def _calculate_confidence_score(self, quality_assessment: EmbeddingQuality, model_used: str) -> float:
        """Calculate overall confidence score for the indexing result"""
        model_performance = self.model_manager.model_performances.get(model_used)
        
        if not model_performance:
            return quality_assessment.quality_score * 0.8
        
        # Weighted combination of factors
        confidence = (
            quality_assessment.quality_score * 0.4 +
            quality_assessment.consistency_score * 0.2 +
            quality_assessment.semantic_density * 0.2 +
            model_performance.accuracy * 0.2
        )
        
        # Penalty for outliers
        if quality_assessment.outlier_score > 2.0:
            confidence *= 0.9
        
        return min(1.0, max(0.0, confidence))
    
    async def search_enhanced(
        self,
        query: str,
        domain: Optional[str] = None,
        limit: int = 10,
        quality_threshold: Optional[float] = None,
        model_preference: Optional[str] = None
    ) -> List[SearchResult]:
        """Enhanced search with quality filtering"""
        try:
            # Select model for query embedding
            query_content = {'text': query, 'domain': domain, 'type': 'query'}
            if model_preference and model_preference in self.model_manager.models:
                query_model = model_preference
            else:
                query_model = self.model_manager.select_optimal_model(query_content)
            
            # Generate query embedding
            query_embedding = await self._generate_embedding(query_content, query_model)
            
            if query_embedding is None:
                logger.error("Failed to generate query embedding")
                return []
            
            # Perform search using base indexer
            if hasattr(self, 'base_indexer'):
                results = self.base_indexer.search(
                    query=query,
                    query_embedding=query_embedding.tolist(),
                    domain=domain,
                    limit=limit * 2  # Get more results to filter by quality
                )
            else:
                results = []  # Fallback
            
            # Filter by quality if threshold specified
            if quality_threshold is not None:
                filtered_results = []
                for result in results:
                    result_quality = result.metadata.get('quality_score', 0.5)
                    if result_quality >= quality_threshold:
                        filtered_results.append(result)
                
                results = filtered_results[:limit]
            
            return results
            
        except Exception as e:
            logger.error(f"Enhanced search failed: {e}")
            return []
    
    def get_model_performance_stats(self) -> Dict:
        """Get performance statistics for all models"""
        stats = {}
        
        for model_key, performance in self.model_manager.model_performances.items():
            stats[model_key] = {
                'model_name': performance.model_name,
                'accuracy': performance.accuracy,
                'speed': performance.speed,
                'quality_score': performance.quality_score,
                'memory_usage': performance.memory_usage,
                'vector_size': performance.vector_size,
                'supported_languages': performance.supported_languages,
                'last_evaluated': performance.last_evaluated.isoformat()
            }
        
        return stats
    
    def optimize_model_selection(self, feedback_data: List[Dict]) -> None:
        """Optimize model selection based on user feedback"""
        logger.info("Optimizing model selection based on feedback...")
        
        # Analyze feedback to adjust model performance scores
        for feedback in feedback_data:
            model_used = feedback.get('model_used')
            user_rating = feedback.get('rating', 0.5)  # 0-1 scale
            content_type = feedback.get('content_type', 'general')
            
            if model_used in self.model_manager.model_performances:
                performance = self.model_manager.model_performances[model_used]
                
                # Adjust accuracy based on feedback (simple learning)
                performance.accuracy = performance.accuracy * 0.9 + user_rating * 0.1
                performance.last_evaluated = datetime.now()
        
        logger.info("Model optimization completed")
    
    def visualize_vector_space(self, collection_name: str, limit: int = 1000) -> Dict:
        """Generate vector space visualization with quality metrics"""
        if not TRANSFORMERS_AVAILABLE:
            return {"error": "Visualization requires advanced ML libraries"}
        
        try:
            # Use base indexer visualization if available
            if hasattr(self, 'base_indexer') and hasattr(self.base_indexer, 'visualize_vector_space'):
                base_viz = self.base_indexer.visualize_vector_space(collection_name, limit)
                
                # Enhance with quality information
                enhanced_viz = {
                    **base_viz,
                    'quality_metrics': {
                        'average_quality': 0.75,  # Would calculate from actual data
                        'quality_distribution': [0.1, 0.2, 0.4, 0.2, 0.1],  # Distribution across quality bins
                        'model_distribution': self._get_model_usage_distribution()
                    }
                }
                
                return enhanced_viz
            
            return {"error": "Visualization not available"}
            
        except Exception as e:
            logger.error(f"Visualization failed: {e}")
            return {"error": str(e)}
    
    def _get_model_usage_distribution(self) -> Dict:
        """Get distribution of model usage"""
        # This would analyze actual usage data
        return {
            'general': 0.4,
            'multilingual': 0.3,
            'semantic': 0.2,
            'domain_academic': 0.1
        }
    
    async def batch_process_enhanced(self, content_list: List[Dict], progress_callback=None) -> List[VectorIndexResult]:
        """Enhanced batch processing with progress tracking"""
        results = []
        total = len(content_list)
        
        for i, content in enumerate(content_list):
            try:
                result = await self.index_content_enhanced(content)
                results.append(result)
                
                if progress_callback:
                    progress_callback(i + 1, total, f"Processed: {content.get('title', 'untitled')}")
                    
            except Exception as e:
                logger.error(f"Failed to process content {i}: {e}")
                continue
        
        logger.info(f"Batch processing completed: {len(results)}/{total} successful")
        return results
    
    def cleanup(self) -> None:
        """Cleanup resources"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
        
        if hasattr(self, 'base_indexer'):
            self.base_indexer.cleanup()


async def main():
    """Example usage of enhanced vector indexer"""
    # Test content
    test_content = {
        'doc_id': 'enhanced_test_123',
        'title': 'Advanced Philosophical Concepts',
        'text': 'This is a test document about complex philosophical ideas and their relationships to modern understanding.',
        'domain': 'philosophy',
        'language': 'en',
        'type': 'academic',
        'metadata': {
            'author': 'Test Author',
            'date': '2024-01-01',
            'source': 'Test Collection'
        }
    }
    
    # Initialize enhanced indexer
    indexer = EnhancedVectorIndexer()
    
    try:
        # Test enhanced indexing
        result = await indexer.index_content_enhanced(test_content)
        print(f"Enhanced indexing result:")
        print(f"- Vector ID: {result.vector_id}")
        print(f"- Model used: {result.model_used}")
        print(f"- Quality score: {result.quality_assessment.quality_score:.3f}")
        print(f"- Confidence: {result.confidence_score:.3f}")
        print(f"- Processing time: {result.processing_time:.2f}s")
        
        if result.quality_assessment.recommendations:
            print("- Recommendations:")
            for rec in result.quality_assessment.recommendations:
                print(f"  * {rec}")
        
        # Test enhanced search
        search_results = await indexer.search_enhanced(
            query="philosophical concepts",
            domain="philosophy",
            limit=5,
            quality_threshold=0.6
        )
        
        print(f"\nFound {len(search_results)} high-quality results")
        
        # Show model performance stats
        stats = indexer.get_model_performance_stats()
        print(f"\nModel performance statistics:")
        for model, perf in stats.items():
            print(f"- {model}: quality={perf['quality_score']:.2f}, speed={perf['speed']:.1f} docs/sec")
        
    except Exception as e:
        print(f"Error: {e}")
    finally:
        indexer.cleanup()


if __name__ == "__main__":
    asyncio.run(main())