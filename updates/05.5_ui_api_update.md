# Phase 5.5: API-First Streamlit Integration
## üîÑ Completing UI Workspace with Existing API Infrastructure

### Overview
This phase focuses on transforming the Streamlit workspace into a thin presentation layer that leverages the existing comprehensive FastAPI backend. This approach eliminates code duplication and maintains clean architectural separation.

### üéØ Objectives
1. **Eliminate Duplication**: Remove all business logic from Streamlit UI
2. **Leverage Existing API**: Use `/api/` endpoints for all operations
3. **Maintain Standards**: Keep all UI files under 500 lines
4. **Enhance User Experience**: Add proper loading states, error handling, and feedback

## üìã Implementation Plan

### Phase 5.5a: API Client Setup (Day 1)

#### 1. Create Unified API Client
**File**: `streamlit_workspace/utils/api_client.py`

```python
"""
Unified API client for Streamlit workspace
Handles all communication with FastAPI backend
"""

import httpx
import streamlit as st
from typing import Dict, List, Optional, Any
import asyncio
from functools import wraps
import json

class APIClient:
    """Centralized API client with error handling and caching"""
    
    def __init__(self):
        self.base_url = st.secrets.get("API_BASE_URL", "http://localhost:8000")
        self.timeout = httpx.Timeout(30.0, connect=5.0)
        self._client = None
    
    @property
    def client(self):
        """Lazy initialization of httpx client"""
        if self._client is None:
            self._client = httpx.AsyncClient(
                base_url=self.base_url,
                timeout=self.timeout,
                headers={"X-Client": "streamlit-workspace"}
            )
        return self._client
    
    async def scrape_content(
        self, 
        urls: List[str], 
        domain: str,
        source_type: str = "webpage",
        options: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Submit content for scraping via API"""
        payload = {
            "urls": urls,
            "domain": domain,
            "source_type": source_type,
            "priority": options.get("priority", 3) if options else 3,
            "subcategory": options.get("subcategory", "") if options else ""
        }
        
        return await self._post("/api/scrape", payload)
    
    async def search_concepts(
        self,
        query: str,
        domain: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict]:
        """Search knowledge graph via API"""
        params = {
            "query": query,
            "limit": limit
        }
        if domain:
            params["domain"] = domain
        
        return await self._get("/api/query/search", params)
    
    async def get_graph_data(
        self,
        concept_id: Optional[str] = None,
        depth: int = 2
    ) -> Dict[str, Any]:
        """Fetch graph visualization data"""
        endpoint = f"/api/graph/concepts/{concept_id}" if concept_id else "/api/graph/overview"
        return await self._get(endpoint, {"depth": depth})
    
    async def manage_database(
        self,
        operation: str,
        data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Database CRUD operations"""
        endpoints = {
            "create": "/api/concepts",
            "update": f"/api/concepts/{data.get('id')}",
            "delete": f"/api/concepts/{data.get('id')}",
            "list": "/api/concepts"
        }
        
        if operation == "list":
            return await self._get(endpoints[operation], data)
        elif operation == "delete":
            return await self._delete(endpoints[operation])
        else:
            return await self._post(endpoints[operation], data)
    
    async def _get(self, endpoint: str, params: Optional[Dict] = None) -> Any:
        """Generic GET request with error handling"""
        try:
            response = await self.client.get(endpoint, params=params)
            response.raise_for_status()
            return response.json()
        except httpx.TimeoutException:
            st.error("‚è±Ô∏è Request timed out. Please try again.")
            return None
        except httpx.HTTPStatusError as e:
            st.error(f"‚ùå API Error: {e.response.status_code}")
            return None
        except Exception as e:
            st.error(f"üö® Unexpected error: {str(e)}")
            return None
    
    async def _post(self, endpoint: str, data: Dict) -> Any:
        """Generic POST request with error handling"""
        try:
            response = await self.client.post(endpoint, json=data)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            st.error(f"üö® Error: {str(e)}")
            return None
    
    async def _delete(self, endpoint: str) -> Any:
        """Generic DELETE request with error handling"""
        try:
            response = await self.client.delete(endpoint)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            st.error(f"üö® Error: {str(e)}")
            return None
    
    def __del__(self):
        """Cleanup client on deletion"""
        if self._client:
            asyncio.create_task(self._client.aclose())

# Singleton instance
api_client = APIClient()

# Async runner for Streamlit
def run_async(async_func):
    """Decorator to run async functions in Streamlit"""
    @wraps(async_func)
    def wrapper(*args, **kwargs):
        return asyncio.run(async_func(*args, **kwargs))
    return wrapper
```

### Phase 5.5b: Refactor Content Scraper (Day 2)

#### 2. Transform Content Scraper to API Client
**File**: `streamlit_workspace/pages/07_üì•_Content_Scraper.py`

```python
"""
Content Scraper - Thin UI layer for scraping operations
Delegates all processing to FastAPI backend
"""

import streamlit as st
from streamlit_workspace.utils.api_client import api_client, run_async
from typing import Dict, List
import json

st.set_page_config(
    page_title="Content Scraper",
    page_icon="üì•",
    layout="wide"
)

class ContentScraperUI:
    """UI-only content scraper interface"""
    
    def __init__(self):
        self.source_configs = self._load_source_configs()
    
    def _load_source_configs(self) -> Dict:
        """Load source type configurations"""
        return {
            "webpage": {
                "icon": "üåê",
                "fields": ["url", "extract_images", "extract_links"]
            },
            "academic_paper": {
                "icon": "üìö",
                "fields": ["url", "doi", "extract_citations", "extract_figures"]
            },
            "ancient_text": {
                "icon": "üìú",
                "fields": ["url", "language", "include_annotations", "translations"]
            },
            "youtube": {
                "icon": "üì∫",
                "fields": ["url", "include_transcript", "include_metadata"]
            }
        }
    
    def render(self):
        """Main render method"""
        st.title("üì• Content Scraper")
        st.markdown("Multi-source content acquisition interface")
        
        # Source type selection
        col1, col2 = st.columns([2, 1])
        
        with col1:
            source_type = st.selectbox(
                "Select Content Source",
                options=list(self.source_configs.keys()),
                format_func=lambda x: f"{self.source_configs[x]['icon']} {x.replace('_', ' ').title()}"
            )
        
        with col2:
            priority = st.select_slider(
                "Priority",
                options=[1, 2, 3, 4, 5],
                value=3,
                help="Higher priority items are processed first"
            )
        
        # Dynamic form based on source type
        self._render_source_form(source_type, priority)
        
        # Batch upload section
        with st.expander("üì¶ Batch Processing"):
            self._render_batch_upload()
        
        # Recent jobs status
        self._render_job_status()
    
    def _render_source_form(self, source_type: str, priority: int):
        """Render form based on source type"""
        config = self.source_configs[source_type]
        
        with st.form(f"{source_type}_form"):
            st.subheader(f"{config['icon']} {source_type.replace('_', ' ').title()} Scraping")
            
            # Common fields
            url = st.text_input("URL", key=f"{source_type}_url")
            domain = st.selectbox(
                "Domain",
                ["mathematics", "science", "philosophy", "religion", "history", "literature"]
            )
            
            # Source-specific fields
            options = {}
            if "doi" in config["fields"]:
                options["doi"] = st.text_input("DOI (optional)")
            
            if "extract_citations" in config["fields"]:
                options["extract_citations"] = st.checkbox("Extract Citations", value=True)
            
            if "language" in config["fields"]:
                options["language"] = st.selectbox(
                    "Original Language",
                    ["Latin", "Greek", "Hebrew", "Sanskrit", "Arabic", "Other"]
                )
            
            if "include_transcript" in config["fields"]:
                options["include_transcript"] = st.checkbox("Include Transcript", value=True)
            
            # Submit button
            submitted = st.form_submit_button("üöÄ Start Scraping")
            
            if submitted and url:
                self._process_scraping(url, domain, source_type, priority, options)
    
    @run_async
    async def _process_scraping(
        self, 
        url: str, 
        domain: str, 
        source_type: str,
        priority: int,
        options: Dict
    ):
        """Process scraping request via API"""
        with st.spinner("üîÑ Processing..."):
            result = await api_client.scrape_content(
                urls=[url],
                domain=domain,
                source_type=source_type,
                options={**options, "priority": priority}
            )
            
            if result and result.get("job_id"):
                st.success(f"‚úÖ Scraping job started: {result['job_id']}")
                st.info("Check the job status below for progress updates")
            else:
                st.error("‚ùå Failed to start scraping job")
    
    def _render_batch_upload(self):
        """Render batch upload interface"""
        uploaded_file = st.file_uploader(
            "Upload CSV/JSON file with URLs",
            type=["csv", "json"]
        )
        
        if uploaded_file:
            # Parse and display preview
            st.info(f"üìÑ File: {uploaded_file.name}")
            # Implementation for file parsing and batch submission
    
    def _render_job_status(self):
        """Display recent job status"""
        st.subheader("üìä Recent Jobs")
        
        # This would fetch from API
        # For now, showing placeholder
        status_placeholder = st.empty()
        status_placeholder.info("Job status will appear here...")

# Initialize and render
scraper_ui = ContentScraperUI()
scraper_ui.render()
```

### Phase 5.5c: Modularize File Manager (Day 3)

#### 3. Create Modular File Manager Package
**Directory Structure**:
```
streamlit_workspace/pages/file_manager/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ main.py              # < 200 lines - Main orchestrator
‚îú‚îÄ‚îÄ csv_manager.py       # < 400 lines - CSV operations
‚îú‚îÄ‚îÄ database_browser.py  # < 400 lines - Database browsing
‚îú‚îÄ‚îÄ api_integration.py   # < 300 lines - API calls
‚îî‚îÄ‚îÄ ui_components.py     # < 300 lines - Reusable UI
```

**File**: `streamlit_workspace/pages/file_manager/main.py`

```python
"""
File Manager - Main orchestrator for database file management
"""

import streamlit as st
from .csv_manager import CSVManager
from .database_browser import DatabaseBrowser
from .api_integration import FileManagerAPI

st.set_page_config(
    page_title="Database File Manager",
    page_icon="üìÅ",
    layout="wide"
)

class FileManagerUI:
    """Main file manager interface"""
    
    def __init__(self):
        self.csv_manager = CSVManager()
        self.db_browser = DatabaseBrowser()
        self.api = FileManagerAPI()
    
    def render(self):
        """Main render method"""
        st.title("üìÅ Database File Manager")
        st.markdown("Manage CSV files and database content")
        
        # Tab selection
        tab1, tab2, tab3 = st.tabs([
            "üìä CSV Files",
            "üóÑÔ∏è Neo4j Browser",
            "üîç Qdrant Collections"
        ])
        
        with tab1:
            self.csv_manager.render()
        
        with tab2:
            self.db_browser.render_neo4j()
        
        with tab3:
            self.db_browser.render_qdrant()

# Initialize and render
if __name__ == "__main__":
    file_manager = FileManagerUI()
    file_manager.render()
```

### Phase 5.5d: Integration Patterns (Day 4)

#### 4. Standardized Integration Patterns

**Pattern 1: Async Operations with Progress**
```python
@run_async
async def process_with_progress(items: List[Any], operation_name: str):
    """Standard pattern for long-running operations"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    results = []
    for i, item in enumerate(items):
        status_text.text(f"Processing {operation_name}: {item}")
        result = await api_client.process_item(item)
        results.append(result)
        progress_bar.progress((i + 1) / len(items))
    
    status_text.text(f"‚úÖ Completed {len(results)} items")
    return results
```

**Pattern 2: Error Handling with Retry**
```python
async def api_call_with_retry(func, max_retries=3):
    """Retry pattern for API calls"""
    for attempt in range(max_retries):
        try:
            return await func()
        except Exception as e:
            if attempt == max_retries - 1:
                st.error(f"Failed after {max_retries} attempts: {str(e)}")
                return None
            await asyncio.sleep(2 ** attempt)  # Exponential backoff
```

**Pattern 3: Caching with TTL**
```python
@st.cache_data(ttl=300)  # 5-minute cache
def fetch_cached_data(endpoint: str, params: Dict):
    """Cache API responses for performance"""
    return asyncio.run(api_client._get(endpoint, params))
```

### Phase 5.5e: Testing Strategy (Day 5)

#### 5. API Integration Tests

**File**: `tests/test_streamlit_api_integration.py`

```python
"""
Test suite for Streamlit-API integration
"""

import pytest
import httpx
from unittest.mock import patch, AsyncMock

class TestAPIIntegration:
    """Test API client integration"""
    
    @pytest.mark.asyncio
    async def test_scrape_content(self):
        """Test content scraping via API"""
        mock_response = {
            "job_id": "test-123",
            "status": "queued"
        }
        
        with patch('httpx.AsyncClient.post', new_callable=AsyncMock) as mock:
            mock.return_value.json.return_value = mock_response
            # Test implementation
    
    @pytest.mark.asyncio
    async def test_error_handling(self):
        """Test error handling in API calls"""
        # Test timeout, HTTP errors, etc.
```

### üìä Migration Timeline

#### Week 1: Core Implementation
- **Day 1**: API client setup and testing
- **Day 2**: Content Scraper refactoring
- **Day 3**: File Manager modularization
- **Day 4**: Graph Editor API integration
- **Day 5**: Testing and bug fixes

#### Week 2: Polish and Optimization
- **Day 1-2**: Performance optimization
- **Day 3**: Error handling improvements
- **Day 4**: Documentation
- **Day 5**: Deployment preparation

### üîß Configuration Updates

#### 1. Streamlit Configuration
**File**: `.streamlit/config.toml`

```toml
[server]
port = 8501
address = "0.0.0.0"
baseUrlPath = ""
enableCORS = false
enableXsrfProtection = true
maxUploadSize = 200

[browser]
gatherUsageStats = false
serverAddress = "localhost"
serverPort = 8501

[theme]
primaryColor = "#1f77b4"
backgroundColor = "#ffffff"
secondaryBackgroundColor = "#f0f2f6"
textColor = "#262730"
```

#### 2. Environment Variables
**File**: `.env.example`

```bash
# API Configuration
API_BASE_URL=http://localhost:8000
API_TIMEOUT=30

# Feature Flags
ENABLE_BATCH_PROCESSING=true
ENABLE_REAL_TIME_UPDATES=true

# Performance
CACHE_TTL=300
MAX_CONCURRENT_REQUESTS=10
```

### üöÄ Deployment Considerations

#### 1. Docker Compose Update
```yaml
version: '3.8'

services:
  streamlit:
    build: ./streamlit_workspace
    ports:
      - "8501:8501"
    environment:
      - API_BASE_URL=http://api:8000
    depends_on:
      - api
    networks:
      - app-network

  api:
    build: ./api
    ports:
      - "8000:8000"
    networks:
      - app-network
    # ... rest of configuration
```

#### 2. Health Checks
```python
# Add to api_client.py
async def health_check() -> bool:
    """Check API availability"""
    try:
        response = await api_client._get("/health")
        return response and response.get("status") == "healthy"
    except:
        return False
```

### üìà Success Metrics

#### Technical Metrics
- [ ] All UI files < 500 lines
- [ ] Zero business logic in UI layer
- [ ] API response time < 500ms
- [ ] 100% API endpoint coverage

#### User Experience Metrics
- [ ] Page load time < 2 seconds
- [ ] Smooth error handling
- [ ] Clear loading states
- [ ] Intuitive navigation

### üéØ Completion Checklist

- [ ] **API Client Implementation**
  - [ ] Base client with error handling
  - [ ] All endpoint methods
  - [ ] Async support
  - [ ] Caching strategy

- [ ] **UI Refactoring**
  - [ ] Content Scraper (API-only)
  - [ ] File Manager (modular)
  - [ ] Graph Editor (API integration)
  - [ ] Database Manager (CRUD via API)

- [ ] **Testing & Documentation**
  - [ ] Integration tests
  - [ ] API documentation
  - [ ] User guide
  - [ ] Deployment guide

### üí° Best Practices Summary

1. **Always use the API client** - Never make direct database calls from UI
2. **Handle loading states** - Users should always know what's happening
3. **Cache intelligently** - Balance freshness with performance
4. **Test edge cases** - Network failures, timeouts, empty responses
5. **Document everything** - Future you will thank present you

---

## Next Steps

After completing Phase 5.5:
1. Run comprehensive integration tests
2. Performance profiling and optimization
3. Security audit of API endpoints
4. Prepare for Phase 6: Advanced Features

**Estimated Completion Time**: 5-6 days of focused development